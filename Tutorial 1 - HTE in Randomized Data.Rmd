---
title: Estimating Heterogeneous Treatment Effects (HTE) in Randomized Data with Machine
  Learning Tecniques
author: "Victor Hugo C. Alexandrino da Silva"
date: "8/11/2021"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\section{1. Goal}

This tutorial discusses and implements different methods to estimate heterogeneous treatment effects (HTE) in \textbf{randomized data}:

\begin{itemize}
\item OLS with interaction terms
\item Post-selection LASSO
\item Honest Causal Tree
\item Causal Forest
\end{itemize}

We will compare the heterogeneity in each of these methods and then compare the \textbf{conditional average treatment effect (CATE)} in each of these methods.

Then, we compare the causal models by the mean square error (MSE).

\section{2. Set-up}

\subsection{2.1. Packages}

First, let's define our directory and install required packages:

```{r, message=FALSE}
# Directory
setwd('~/Google Drive/PhD Insper/Thesis/Paper 3/Empirics/Tutorials/HTE Randomized')

# Packages
library(glmnet)               # LASSO
library(rpart)    
library(rpart.plot)
library(randomForest)         # Random Forest
library(devtools)             # GitHub installation
library(tidyverse)    
library(ggplot2)
library(dplyr)
library(grf)                  # Generalized Random Forests
#install_github('susanathey/causalTree')
library(causalTree)           # Causal Tree 
#install_github('swager/randomForestCI')
library(randomForestCI)
# install_github('swager/balanceHD')
library(balanceHD)            
library(SuperLearner)
# install.packages("caret")
library(caret)
# install.packages("xgboost")
library(xgboost)
```


\subsection{2.2. Data}

We used data from the article “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment” by Gerber, Green and Larimer (2008). In a random experiment, the article study the effect of letters encouraging voters in the amount of voter turnout. The paper estimated that there is an average of 8 p.p. increase in turnout. 


Data is splitted in our set of variables $Z = \{Y_i,X_i,W_i\}$, where $Y$ is the outcome variable (voted or not), $X$ is the treatment variable (received letter or not) and $W$ the covariates. 

```{r}
# Load data
my_data <- readRDS('social_voting.rds')

#Restrict the sample size
n_obs <- 33000 # Change this number depending on the speed of your computer. 6000 is also fine. 
my_data <- my_data[sample(nrow(my_data), n_obs), ]

# Split data into 3 samples
folds = createFolds(1:nrow(my_data), k=2)

Y_train <- my_data[folds[[1]],1]
Y_test <- my_data[folds[[2]],1]

X_train <- my_data[folds[[1]],2]
X_test <- my_data[folds[[2]],2]

W_train <- my_data[folds[[1]],3:ncol(my_data)]
W_test <- my_data[folds[[2]],3:ncol(my_data)]

### Creates a vector of 0s and a vector of 1s of length n (hack for later usage)
zeros <- function(n) {
  return(integer(n))
}
ones <- function(n) {
  return(integer(n)+1)
}

```


\section{3. CATE, Causal trees and causal forests}

\subsection{3.1. OLS with interaction terms}

We first estimate our CATE using the standard OLS. Let's start with a simple OLS with interaction terms. This is a simple way to estimate differential effects of $X$ on $Y$, where we only need to include interaction terms in an OLS regression. The algorithm follows:

\begin{itemize}
\item i) Regress $Y$ on $X$ and $W$
\item ii) Interact $X$ and $W$ in order to find heterogeneous effects of $X$ on $Y$ depending on $W$.  
\end{itemize}

Thus, we model an OLS model with interactions as the following:

$$ Y = \beta_0 + \beta_1 X + \beta_2 W + \beta_3 (W \times X) + \varepsilon $$
Where $X$ is the treatment vector and $W$ the covariates. 

We will use the R package `SuperLearner` to implement some of our ML algorithms:

```{r}
# Estimate a linear model algorithm
sl_lm = SuperLearner(Y = Y_train,
                     X = data.frame(X=X_train, W_train , W_train*X_train),
                     family = binomial(),              # Distribution of errors
                     SL.library = "SL.lm",             # Linear model
                     cvControl = list(V=0))            # Method for cross-validation

summary(sl_lm$fitLibrary$SL.lm_All$object)

```

Thus, note that the treatment $X$ itself has a positive statistically significant effect of 0.064 in the vote turnout. However, we observe statistically significant heterogeneous effect when we interact with the 2004-vote dummy, the percent_asian, the median income variable. It seems that the effect is heterogeneous for some of our covariates. Moreover, a bunch of features seems to be relevant for our linear model, which reinforces the need to estimate the CATE instead of the ATE. That is why we need one step forward.

\subsubsection{3.1.1. CATE for OLS}

We can simply predict our outcome for both treated and non-treated groups in order to estimate the CATE:

$$ CATE = \hat{\tau} = E(Y|X = 1,W) - E(Y|X=0,W) $$
For this, we use the `predict` function in our `sl_lm` model for each group, using our test sample:

```{r}
# Prediction on control group (X = 0)
ols_pred_control <- predict(sl_lm, data.frame(X = zeros(nrow(W_test)), W_test, W_test*zeros(nrow(W_test))), onlySL = T)

# Prediction on treated group (X = 1)
ols_pred_treated <- predict(sl_lm, data.frame(X = ones(nrow(W_test)), W_test, W_test*ones(nrow(W_test))), onlySL = T)

# Calculate CATE
cate_ols <- ols_pred_treated$pred - ols_pred_control$pred

plot(cate_ols)

# Calculate ATE
mean(cate_ols)

plot(mean(cate_ols))
```


\subsection{3.2. Post-selection LASSO}

Now, we use LASSO to estimate the heterogeneous effects. However, before estimating the CATE, we use it as a screening algorithm. What does it mean? That in order to reduce the number of variables, we can use LASSO to select the relevant variables. We will use the SuperLearner library again:

```{r}
# Defining LASSO
lasso = create.Learner("SL.glmnet",
                       params = list(alpha = 1),
                       name_prefix = "lasso")


# Getting coefficients by LASSO
get_lasso_coeffs <- function(sl_lasso) {
  return(coef(sl_lasso$fitLibrary$lasso_1_All$object, se = "lambda.min")[-1,])
}


SL.library <- lasso$names

predict_y_lasso <- SuperLearner(Y = Y_train,
                                X = data.frame(X = X_train, W_train ,W_train*X_train),
                                family =  binomial(),
                                SL.library = SL.library,
                                cvControl = list(V=0))

kept_variables <- which(get_lasso_coeffs(predict_y_lasso)!=0)


predict_x_lasso <- SuperLearner(Y = X_train,
                                X = data.frame(W_train),
                                family = binomial(),
                                SL.library = lasso$names,
                                cvControl = list(V=0))


kept_variables2 <- which(get_lasso_coeffs(predict_x_lasso)!=0) + 1

```

After selecting the variables by LASSO, we can use the OLS to estimate treatment heterogeneity in the relevant variables selected by the algorithm above. But first, let's formulate our post selection OLS:

```{r}
sl_post_lasso <- SuperLearner(Y = Y_train,
                              X = data.frame(X = X_train, W_train, W_train*X_train)[,c(kept_variables,kept_variables2)],
                              family = binomial(),
                              SL.library = "SL.lm",
                              cvControl = list(V=0))

summary(sl_post_lasso$fitLibrary$SL.lm_All$object)

```
That is, now, after selecting by LASSO our most relevant variables, we note heterogeneity only in the p2004 dummy. However, we have a set of relevant variables different from the treatment that are also statistically significant. 

What is remaining is our estimation of CATE for post-selection LASSO. We can code it as the following, using the test sample:

```{r}
# Prediction on control group (X = 0)
postlasso_pred_control <- predict(sl_post_lasso, data.frame(X = zeros(nrow(W_test)), W_test, W_test*zeros(nrow(W_train)))[,c(kept_variables,kept_variables2)], onlySL = T)

# Prediction on control group (X = 1)
postlasso_pred_treated <- predict(sl_post_lasso, data.frame(X = ones(nrow(W_test)), W_test, W_test*ones(nrow(W_test)))[,c(kept_variables,kept_variables2)], onlySL = T)

# Estimating CATE with post-selection LASSO
cate_postlasso <- postlasso_pred_treated$pred - postlasso_pred_control$pred

# Plot cate_postlasso
plot(cate_postlasso)

# ATE
mean(cate_postlasso)

# Plot ATE
plot(mean(cate_postlasso))

```
\section{3.3. Causal Trees}

Now we are going to predict the CATE from tree-based algorithms. There are a few packages that do this, but I like Susan Athey's `causalTree` and `grf`. The first is a general algorithm that builds a regression model and returns an \textit{rpart} object, implementing ideas from the CART (Classification and Regression Trees), by Breiman et al. The second implements the generalized random forest algorithm for causal forest, which uses a splitting tree-based rule to divide covariates based in the heterogeneity of treatment effects and, moreover, assumes honesty as one of main assumptions. 

In summary, we want to model

$$ Y_i = W_i + \theta X_i + \varepsilon $$
Where $c$ are possible covariates that brings hetrogeneity on the treatment $X_i$ for the outcome $Y_i$. The algorithm finds

$$ \hat{\tau}(W) = argmin_\tau \alpha_i(W) W_i(Y_i - \tau X_i)^2 $$
Where the weights $\alpha_i$ are estimated by a random forest algorithm. Let's begin by building our causal tree:

```{r}
# Witting the regression formula (to facilitate later)
tree_fml <- as.formula(paste("Y", paste(names(W_train), collapse = ' + '), sep = " ~ "))

# Building causal tree
causal_tree <- causalTree(formula = tree_fml,
                          data = data.frame(Y = Y_train,W_train),
                          treatment = X_train,
                          split.Rule = "CT",           # Causal Tree
                          split.Honest = FALSE,        # So far, we are not assuming honesty
                          split.alpha = 1,
                          cv.option = "CT",
                          cv.Honest = FALSE,
                          split.Bucket = TRUE,
                          bucketNum = 5,
                          bucketMax = 100,
                          minsize = 250                # Number of obs in treatment and control on leaf
                          )

rpart.plot(causal_tree, roundint = FALSE)

```
\subsubsection{3.3.1. Honest Causal Trees}

Honest trees can be obtained when we use part of the sample (train) for building the leafs and part (test) to calculate the heterogeneity of treatment effects. The function `honest.causalTree` does the job:

```{r}
honest_tree <- honest.causalTree(formula = tree_fml,
                                 data = data.frame(Y=Y_train, W_train),
                                 treatment = X_train,
                                 est_data = data.frame(Y=Y_test, W_test),
                                 est_treatment = X_test,
                                 split.alpha = 0.5,
                                 split.Rule = "CT",
                                 split.Honest = TRUE,
                                 cv.alpha = 0.5,
                                 cv.option = "CT",
                                 cv.Honest = TRUE,
                                 split.Bucket = TRUE,
                                 bucketNum = 5,
                                 bucketMax = 100, # maximum number of buckets
                                 minsize = 250) # number of observations in treatment and control on leaf

rpart.plot(honest_tree, roundint = F)
```

Then, we prune the tree with cross validation, choosing the simplest tree that minimizes the objective function in a left-out sample:

```{r}
opcpid <- which.min(honest_tree$cp[, 4]) 
opcp <- honest_tree$cp[opcpid, 1]
honest_tree_prune <- prune(honest_tree, cp = opcp)

rpart.plot(honest_tree_prune, roundint = F)
```

That is, we have spitted our sample in order to maximize the heterogeneity in each node. That is the way these algorithms work: By splitting in sub-samples by heterogeneity, we are able to estimate with more accuracy the treatment effect. 

To estimate the standard errors on the leaves, we can use an OLS. The linear regression is specified such that the coefficients on the leaves are teh treatment effects. 

```{r}
# Constructing factors variables for the leaves
leaf_test <- as.factor(round(predict(honest_tree_prune,
                                     newdata = data.frame(Y = Y_test, W_test),
                                     type = "vector"), 4))

# Run an OLS that estimate the treatment effect magnitudes and standard errors
honest_ols_test <- lm(Y ~ leaf + X * leaf - X -1, data = data.frame(Y = Y_test, X = X_test, leaf = leaf_test, W_test))

summary(honest_ols_test)

```

That is, only the leaf on median income is not statistically significant. All other heterogeneities are relevant in our honest tree. 

But we still need to predict our CATE from our tree. This is easily done by the function predict:

```{r}
# Estimate CATE
cate_honesttree <- predict(honest_tree_prune, newdata = data.frame(Y = Y_test, W_test), type = "vector")

# Plot CATE
plot(cate_honesttree)

# ATE
mean(cate_honesttree)

# Plot ATE
plot(mean(cate_honesttree))
```

\subsection{3.4. Causal Forests}

Finally, we estimate the CATE with the causal forest algorithm. The method is similar to the R-learner, but with a splitting procedure using a tree-based algorithm. It uses a residual-residual approach do estimate the propensity score in order to find the conditional average treatment effect. For more information, check Jacob (2021).

Let's do in two ways. The first one will be using the `grf` package that estimates the CATE directly in the function. The second uses the `causalForest` package, estimating a random forest with honest causal trees. 

\subsection{3.4.1. Generalized Random Forest}

The `grf` algorithm assumes honesty as the main assumption. We can easily do this by fitting the causal forest with our training sample and predicting with our testing sample. Estimating our causal forest is simply:

```{r}
# Train our causal forest
cf_grf <- causal_forest(X = W_train,
                        Y = Y_train,
                        W = X_train)

# Get predictions from test sample
effects_cf_grf <- predict(cf_grf,W_test)

# Get effects
effects_cf_grf_pred <- predict(cf_grf,W_test)$predictions

# Histogram
hist(effects_cf_grf_pred)

# Plot of treatment effects
plot(W_test[, 1], effects_cf_grf_pred, ylim = range(effects_cf_grf_pred, 0, 2), xlab = "x", ylab = "tau", type = "l")

# Estimate the CATE for the full sample
cate_grf <- average_treatment_effect(cf_grf, target.sample = "all")

cate_grf

# Estimate the CATE for the treated sample (CATT)
average_treatment_effect(cf_grf, target.sample = "treated")

# Best Linear Projection of the CATE
cate_best_grf <- best_linear_projection(cf_grf)

cate_best_grf
```

That is, our estimated CATE is around 0.10, both when we predict the result using the `average_treatment_effect` function or when we use the `best_linear_projection`. Let's see if it holds without the `grf` package.

\subsubsection{3.4.2. causalTree package}

The same estimation can be obtained by the function `causalForest` inside the `causalTree` package:

```{r}
cf_causalTree <- causalForest(tree_fml,
                             data=data.frame(Y=Y_train, W_train), 
                             treatment=X_train, 
                             split.Rule="CT", 
                             split.Honest=T,  
                             split.Bucket=T, 
                             bucketNum = 5,
                             bucketMax = 100, 
                             cv.option="CT", 
                             cv.Honest=T, 
                             minsize = 2, 
                             split.alpha = 0.5, 
                             cv.alpha = 0.5,
                             sample.size.total = floor(nrow(Y_train) / 2), 
                             sample.size.train.frac = .5,
                             mtry = ceiling(ncol(W_train)/3), 
                             nodesize = 5, 
                             num.trees = 10, 
                             ncov_sample = ncol(W_train), 
                             ncolx = ncol(W_train))
```

And, to estimate the CATE:

```{r}
cate_causalTree <- predict(cf_causalTree, newdata = data.frame(Y = Y_test, W_test), type = "vector")
```


\section{4. Comparing models}

Finally, we can compare all of our models (OLS with interaction terms, post-selection LASSO, Causal trees and Causal forest). Let's first see some histograms:

```{r}
# Creating a data frame withh all CATEs
het_effects <- data.frame(ols = cate_ols,
                          post_selec_lasso = cate_postlasso,
                          causal_tree = cate_honesttree,
                          causal_forest_grf = effects_cf_grf,
                          causal_forest_causalTree = cate_causalTree)

# Set range of x-axis
xrange <- range(c(het_effects[,1],het_effects[,2],het_effects[,3],het_effects[,4],het_effects[,5]))

# Set margins (two rows, five columns)
par(mfrow = c(1,5))

hist(het_effects[, 1], main = "OLS", xlim = xrange)
hist(het_effects[, 2], main = "Post-selection Lasso", xlim = xrange)
hist(het_effects[, 3], main = "Honest Causal tree", xlim = xrange)
hist(het_effects[, 4], main = "GRF Causal forest", xlim = xrange)
hist(het_effects[, 5], main = "causalTree Causal forest", xlim = xrange)
```

And to finalize, we can summary a table of results with each of CATEs:

```{r}
summary_stats <- do.call(data.frame, 
                         list(mean = apply(het_effects, 2, mean),
                              sd = apply(het_effects, 2, sd),
                              median = apply(het_effects, 2, median),
                              min = apply(het_effects, 2, min),
                              max = apply(het_effects, 2, max)))

summary_stats
```
From the histograms and the summary statistics, it seems that the causal forest from the `grf` package yields most heterogeneity, once we have higher standard deviation among our treatment effects.

We can also compare the methods by looking at the minimum square error (MSE) on a test set using the transformed outcome (which I call $Y^*$) as a proxy for the true treatment effect. For this, we need to construct the propensity score ($E(X = 1)$) of our treatment in our test sample. Let's code it:

```{r}
# Construct propensity score from randomized experiment
prop_score <- mean(X_test)

# Construct Y_star in our test sample
Y_star <- X_test * (Y_test / prop_score) - (1 - X_test) * (Y_test / (1 - prop_score))

## MSEs

# OLS with interaction
MSE_ols <- mean((Y_star - cate_ols)^2)

# Post-selection LASSO
MSE_lasso <- mean((Y_star - cate_postlasso)^2)

# Honest Tree
MSE_causalTree <- mean((Y_star - cate_honesttree)^2)

# Causal Forest GRF
MSE_cf_grf <- mean((Y_star - cate_grf)^2)

# Causal Forest causalTree
MSE_cf_causalTree <- mean((Y_star - cate_causalTree)^2)

# Create data frame with all MSEs
performance_MSE <- data.frame(matrix(rep(NA,1), nrow = 1, ncol = 1))
rownames(performance_MSE) <- c("OLS")
colnames(performance_MSE) <- c("MSE")

# Load in results
performance_MSE["OLS","MSE"] <- MSE_ols
performance_MSE["Post-selection LASSO","MSE"] <- MSE_lasso
performance_MSE["Honest Tree","MSE"] <- MSE_causalTree
performance_MSE["Causal Forest GRF","MSE"] <- MSE_cf_grf
performance_MSE["Causal Forest causalTree","MSE"] <- MSE_cf_causalTree

# Setting range
xrange2 <- range(performance_MSE$MSE - 2*sd(performance_MSE$MSE), 
                 performance_MSE$MSE,
                 performance_MSE$MSE + 2*sd(performance_MSE$MSE))

# Create plot
MSEplot <- ggplot(performance_MSE) + 
  geom_bar(mapping = aes(x = factor(rownames(performance_MSE), 
                                    levels = rownames(performance_MSE)), 
                         y = MSE),
           stat = "identity", fill = "gray44", width=0.7, 
           position = position_dodge(width=0.2)) + 
  theme_bw() + 
  coord_cartesian(ylim=c(xrange2[1], xrange2[2])) +
  theme(axis.ticks.x = element_blank(), axis.title.x = element_blank(),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        plot.background = element_blank(),
        axis.text.x = element_text(angle = -90, hjust = 0, vjust = 0.5)) +
  ylab("MSE out-of-sample") + 
  ggtitle("Comparing performance based on MSE") +
  theme(plot.title = element_text(hjust = 0.5, face ="bold", 
                                  colour = "black", size = 14))

# Plot
MSEplot

```

From the MSE analysis, it seems that both causal forest models perform worse than simpler models like OLS, post-selection LASS) and honest tree. One explanation could be that since we have little heterogeneity in our model, simpler models do best. 

\section{References}

https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html

https://gsbdbi.github.io/ml_tutorial/

https://ml-in-econ.appspot.com

https://github.com/QuantLet/Meta_learner-for-Causal-ML/blob/main/GRF/Causal-Forest.R

https://grf-labs.github.io/grf/

https://www.markhw.com/blog/causalforestintro

https://lost-stats.github.io/Machine_Learning/causal_forest.html






