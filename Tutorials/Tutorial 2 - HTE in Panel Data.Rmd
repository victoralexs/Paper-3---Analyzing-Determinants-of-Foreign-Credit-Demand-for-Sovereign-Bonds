---
title: "Tutorial 2 - Estimating Heterogeneous Treatment Effects in Panel Data with Machine Learning Techniques"
author: "Victor Hugo C. Alexandrino da Silva"
date: "8/18/2021"
output:
  html_document:
    df_print: paged
    keep_md: yes
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a tutorial that extends \textit{Tutorial 1 - Estimating Heterogeneous Treatment Effects in Randomized Data with Machine Learning Techniques} with panel data. The idea here is to provide comparisons between three different data process using the `grf` package for Causal Forest algorithms. 

There are three different data generating processes:

\begin{itemize}
\item \textbf{1. Linear Interaction:} A linear interaction between a variable of interest and the treatment dummy. 
\item \textbf{2. Non-linear interaction:} A non linear function of a variable of interest $X$ and a treatment dummy $Y$.
\item \textbf{3. Log non-linear interaction:} A non-linear function of the variable $X$ in log terms and a treatment variable $W$.
\end{itemize}

\section{1. Linear Interaction}

Let's first create our panel data setting. The model has a treatment variable $W_{i t}$ and a set of covariates $V1_{i t}$ for each individual firm $i$ and time $t$. Then, our panel model can be written as

$$ y_{i t} = \alpha_i + V1_{i t} + W_{i t} + V1_{i t} \times W_{i t} $$

Where $\alpha_{i}$ are non-observed individual fixed effects, correlated with the covariates set $V1$.

As a linear case, let's first run a fixed effect model, together with a random effect model and an OLS for panel data. Let's begin with our packages:

```{r, message=FALSE}
library(lfe, quietly = TRUE) # Linear Group Fixed Effects
library(lme4)                # Linear Mixed Models
require(snowfall)            # Cluster programming
library(MASS)
library(grf)                # Generalized Random Forest
library(tidyverse)          # Data manipulation
library(plm)
```

Now, we create our dataset. As the standard of machine learning models and since the causal forest algorithm assumes honest trees besides the causality assumptions, we split our dataset in train and test sample.

```{r, message=FALSE, warning=FALSE}
set.seed(123)
rm(list = ls())

# Number of periods
t <- 10

# Number of variables
p <- 10

# Number of firms
n <- 400

# Generating p random variables
data1 = as.data.frame(matrix(rnorm(n*t*p), n*t, p))

firm = seq(1,n)

time = seq(1,t)

data <- expand.grid(firm = firm, time = time)

# Defining the treatment assignment
data$W <- rbinom(n*t, 1, 0.5)

# Defining train sample
data$train <- rbinom(n*t, 1, 0.5)

# Generate two correlated variables (V1 and V2)
covarMat = matrix( c(1, .95^2, .95^2, 1 ) , nrow=2 , ncol=2 )
data.2  = as.data.frame(mvrnorm(n=n*t , mu=rep(0,2), Sigma=covarMat ))

data <- bind_cols(data,data.2) %>%
    tbl_df()

summary(data$firm)

data1$V1 <- NULL

data1$V2 <- NULL

#colnames(data1) <- c("var1","var2","var3","var4","var5","var6","var7","var8","var9","var10")

data <- bind_cols(data,data1)

#data <- subset(data, )

# Generate unit (fixed) effect by firm means of V2, which is correlated with V1
data <- data %>%
    group_by(firm) %>%
    mutate(unit.effect=mean(V2)) %>%
    ungroup() %>%
    arrange(firm, time)

summary(data$firm)

# Generate outcome variable y and fixed unit effects  index
y<-c()
unit<-c()
X<-c()
k <- 0
for(i in 1:n){
    for(j in 1:t){
        k<-k+1
        unit[k]<-i
        y[k]<-1+ data$V1[k] + data$W[k] + data$V1[k]*data$W[k] + data$unit.effect[k]+rnorm(1,mean=0,sd=1)
    }
}
data$unit=unit
data$y=y

# Split between train and test set
data.train <- data %>%
    filter(train==1)
data.test <- data %>%
    filter(train==0)

summary(data.train)

summary(data.train$firm)
summary(data.test$firm)
summary(data$firm)

summary(data$time)
summary(data.train$time)
summary(data.test$time)

#setdiff(data.train,data.test)

```


Now, let's run our FE, RE and LPM model to see whether they are biased. Remember that the individual fixed effect $\alpha_i$ (variable `unit_effect`) is highly correlated with the covariate $V1$, built from $V1$ means. Moreover, we estimate in our train data.

```{r}
## Fixed Effects
fe_felm <- felm(y ~ V1*W | unit, data = data.train)

# With plm package
fe_plm <- plm(y ~ V1*W, data = data.train, index = c("unit"), model = "within")


## Random Effects
re_lmer <- lmer(y ~V1*W+(1 | unit), data = data.train)

# With plm package
re_plm <- plm(y ~V1 * W, data = data.train, index = c("unit"), model = "random")


## POLS

ols <- lm(y ~ V1*W, data=data.train)


## Summary

summary(fe_felm)
summary(fe_plm)
summary(re_lmer)
summary(re_plm)
summary(ols)

```

We observe that our treatment variable is statically significant in all estimators, but there is a bias both in RE and OLS models due to non-observable effects. 

Now, let's run a Causal Forest with the `grf` package including all other variables in the data set. However, first we need to deal with the panel structure. The solution is to include all other variables in the data set but not $V2$ (since it was used to generate the fixed effect $\alpha_i$) together with all the firm dummies using the train sample:

```{r}
# Creating the dataset of firm dummies
firm_dummies <- as.data.frame(model.matrix(y ~ as.factor(firm), data.train))

#summary(firm_dummies)

# Separating between covariates, outcome and tratment, merging with the firm dummies

# Features (obs: we exclude V2 since it is correlated with fixed effects by our data structure)
X <- as.matrix(bind_cols(data.train[,c(5,7:16)],firm_dummies))

# Outcome
Y <- data.train$y

# Treatment
W <- data.train$W

# Running our causal forest. Recall that, in order to account firm fixed effects, we have created firm dummies as categorical variables
tau.forest <- causal_forest(X,Y,W)

# Estimate ATE for the full sample
average_treatment_effect(tau.forest, target.sample = "all")

# Estimate ATE for treated sample
average_treatment_effect(tau.forest, target.sample = "treated")

# Best Linear Projection for CATE
cate_best <- best_linear_projection(tau.forest)

cate_best

```

That is, our ATE estimated by the causal forest algorithm is quite similar with the ATE estimated by the linear model, especially when we account for firm fixed effects. Moreover, the best linear projection from the `grf` package also predicts similar values for the ATE.

We did the ATE prediction in the train sample. However, since the algorithm assumes honesty, we should do it at the test sample. For a full explanation, check Tutorial 1. 

Therefore, 

```{r}
# Creating firm dummies for test sample
firm_dummies_test <- as.data.frame(model.matrix(y ~ as.factor(firm), data.test))

#setdiff(firm_dummies_test,firm_dummies)

#summary(firm_dummies_test)

# Creating each X, Y and W from test sample

# Features
X.test <- as.matrix(bind_cols(data.test[,c(5,7:16)],firm_dummies_test))

 # Outcome
Y.test <- data.test$y

# Treatment
W.test <- data.test$W

# Predicting CATE using the causal forest (trained by the train sample) and the test sample (for some reason, there is an error in number of columns of firm dummies for the test sample. Let's predict for the train sample)
tau.hat <- predict(tau.forest, estimate.variance = TRUE)

# Predicting variance
sigma.hat = sqrt(tau.hat$variance.estimates)

# Extracting CATE
data.train$pred <- tau.hat$predictions

# Histogram of CATE
hist(data.train$pred)

# Plotting CATE with respect to the covariate V1
ggplot(data.train, aes(x=V1, y=pred)) + geom_point()



```
That is, this is the predicted value of CATE in our train data compared with the value of $V1$, our covariate that is correlated with the firm fixed effects (which was created from $V2$). It seems that there is a positive linear relationship between the feature $V1$ and the predicted CATE. This was true when we predicted our linear models.

Now, let's graph the predicted $CATE$ against the true one. We expect them to be similar:

```{r}

# Create dataframe with predicted CATE info
graph.data <- as.data.frame(tau.hat)

# Create variance observations for CATE for each observation and confidence interval bounders
graph.data <- graph.data %>% 
    mutate(cate = 1 + data.train$V1,0) %>% 
    mutate(lower = predictions - sigma.hat, upper = predictions + sigma.hat)

# Plot CATE prediction x CATE true
ggplot(graph.data , aes(x=cate, y=predictions)) + geom_point() 

# And finally, plotting error bar from CI:
ggplot() + geom_errorbar(graph.data , mapping=aes(x=cate,  ymin=lower,ymax=upper), alpha = 0.2) + geom_point(graph.data , mapping=aes(x=cate,  y=predictions), alpha = 0.2)


```

In general, our both CATE and ATE estimates are similar in the linear models and the causal forest algorithm. Let's look at the power of the causal forest algorithm when comparing with the non-linear estimation.

\section{2. Non-linear interaction}

Assume now that our ATE/CATE assuming that we now have a non-linear model:

$$ y_{i t} = 1 + \alpha_i + \max(V1_{i t},0)*W_{i t} + \varepsilon_{i t}$$
Let's re-create our dataset with the non-linear relationship between $V_1$ and $W$:

```{r, message=FALSE, warning=FALSE}
set.seed(321)
rm(list = ls())

# Number of periods
t <- 10

# Number of variables
p <- 10

# Number of firms
n = 400

# Generate the random variables
data1 = as.data.frame(matrix(rnorm(n*t*p), n*t, p))

firm = seq(1,n)

time = seq(1,t)

data <- expand.grid(firm = firm, time = time)

# Treatment variable
data$W <- rbinom(n*t,1,0.5)

# Train dummy
data$train <- rbinom(n*t,1,0.5)

# Generate two correlated variables V1 and V2
covarMat = matrix( c(1, .95^2, .95^2, 1 ) , nrow=2 , ncol=2 )
data.2  = as.data.frame(mvrnorm(n=n*t , mu=rep(0,2), Sigma=covarMat ))

data <- bind_cols(data,data.2) %>% 
    tbl_df()

data1$V1 = NULL

data1$V2 = NULL

data <- bind_cols(data,data1)

# Generate fixed effects by firm means of V2, correlated with V1:
data <- data %>%
    group_by(firm) %>%
    mutate(unit.effect=mean(V2)) %>%
    ungroup() %>%
    arrange(firm, time)

# Create non-linear outcome and fixed effects index
y<-c()
unit<-c()
X<-c()
k <- 0
for(i in 1:n){
    for(j in 1:t){
        k<-k+1
        unit[k]<-i
        y[k]<-1+ pmax(data$V1[k],0)*data$W[k]  + data$unit.effect[i]+rnorm(1,mean=0,sd=1)
    }
}
data$y=y
data$unit <- unit

# Split between train and test sample
data.train <- data %>% 
    filter(train==1)

data.test <- data %>% 
    filter(train==0)
```

Now, let's run our linear models:

```{r}
# Fixed effects
fe <- felm(y ~V1*W | unit, data = data.train)

# Random effects
re <- lmer(y ~ V1*W + (1 | unit), data = data.train)

# POLS
ols <- lm(y ~ V1 * W, data = data.train)

# Summary for all linear models
summary(fe)
summary(re)
summary(ols)
```

Note that in our non-biased estimation, the covariate $V1$ is no longer statistically significant. But we have heterogeneity from the interaction and the treatment $W$ seems to be relevant, but with small goodness of fit variables.

One solution for these problems in non-linear relations between treatment, covariate and outcome is the causal forest algorithm. Let's predict first the CATE against $V1$, as we did in the linear case.

```{r}
# Setting firm dummies for the train sample
firm_dummies <- as.data.frame(model.matrix(y~as.factor(firm),data.train))

# Splitting our train sample
X <- as.matrix(bind_cols(data.train[,c(5,7:16)],firm_dummies))
Y <- data.train$y
W <- data.train$W

# Train the causal forest
tau.forest2 <- causal_forest(X,Y,W)

# Estimating ATE from the full train sample
average_treatment_effect(tau.forest2, target.sample = "all")

# Estimating ATE from the treated train sample
average_treatment_effect(tau.forest2, target.sample = "treated")

# Estimating CATE with Best Linear Projection
cate_best_nl <- best_linear_projection(tau.forest2)

cate_best_nl

```

Now, let's create our test sample in order to predict the CATE:

```{r}
# Create firm dummies in the test sample
firm_dummies_test <- as.data.frame(model.matrix(y~as.factor(firm),data.test))

X.test <- as.matrix(bind_cols(data.test[,c(5,7:16)],firm_dummies_test))
Y.test <- data.test$y
W.test <- data.test$W

# Predicting CATE with the test sample
tau.hat2 = predict(tau.forest2, X.test, estimate.variance = TRUE)

# Defining standard errors from CATE
sigma.hat2 <- sqrt(tau.hat2$variance.estimates)

# Creating the column "pred2" for the CATE predictions in the test sample
data.test$pred2 <- tau.hat2$predictions
```

The next step is to plot our CATE. Let's first see the relationship between the covariate $V1$ with the predicted CATE:

```{r}
# Plot CATE vs V1
ggplot(data.test,aes(x = V1, y = pred2)) + 
    geom_point()
```

Note that in fact there is a non-linear relationship between $V1$ and the predicted CATE in our covariates. 

Now, let's plot the predicted CATE with the real CATE:

```{r}
# Storing out CATE info in a dataframe
graph.data2 <- as.data.frame(tau.hat2)

# Create cate and CI bounders
graph.data2 <- graph.data2 %>%
    mutate(cate=pmax(data.test$V1,0)) %>%
    mutate(lower=predictions - sigma.hat2, upper=predictions + sigma.hat2)

# Plot the relationship between the predicted CATE and the real CATE
ggplot(graph.data2, aes(x=cate, y=predictions)) + geom_point() 

```

And finally, our error plot:

```{r}
# Plot error plot with standard errors
ggplot() + geom_errorbar(graph.data2, mapping=aes(x=cate,  ymin=lower,ymax=upper), alpha = 0.2) + geom_point(graph.data2, mapping=aes(x=cate,  y=predictions), alpha = 0.2)
```

It seems that now we have a better estimation of the ATE/CATE when there are non-linearities between our treatment and covariates. Moreover, the prediction performs really well, with close estimations of the coefficients.

\section{3. Log non-linear interaction}

Finally, we can see that the causal forest algorithm also performs really well when the non-linearity comes from the log relation. This is useful, for example, when we account for GDP growth in macro panels, or level-log models that we know that there is no linear relationship between the feature and treatment.


Assume now that our model is such that

$$ y_{i t} = 1 + \alpha_{i} + \log(V1_{i t}^2)* W_{i t} + \varepsilon_{i t} $$
But we observe only $V1_{i t}$, not $\log(V1_{i t}^2)$.

Let's again create our panel with $V1$ being correlated with $V2$ and the fixed effect `unit`:

```{r, message = FALSE, warning=FALSE}
set.seed(666)

rm(list=ls())

# Number of periods
t <- 10

# Number of covariates
p <- 10

# Number of firms
n = 400

# Generate random covariates
data1 = as.data.frame(matrix(rnorm(n*t*p), n*t, p))

firm = seq(1,n)

time = seq(1,t)

data <- expand.grid(firm=firm,time=time)

# Create treatment variable
data$W <- rbinom(n*t,1,0.5)

# Create train sample indicator
data$train <- rbinom(n*t,1,0.5)

# Generate V1 and V2, two correlated covariates
covarMat = matrix( c(1, .95^2, .95^2, 1 ) , nrow=2 , ncol=2 )
data.2  = as.data.frame(mvrnorm(n=n*t , mu=rep(0,2), Sigma=covarMat ))

data <- bind_cols(data,data.2) %>% 
    tbl_df()

data1$V1 = NULL
data1$V2 = NULL 

data <- bind_cols(data,data1)

# Generate treatment effects by firm means of V2, correlated with V1
data <- data %>%
    group_by(firm) %>%
    mutate(unit.effect=mean(V2)) %>%
    ungroup() %>%
    arrange(firm, time)

# Create non-linear outcome
y<-c()
unit<-c()
X<-c()
k <- 0

for(i in 1:n){
    for(j in 1:t){
        k<-k+1
        unit[k]<-i
        y[k]<-1+ log(data$V1[k]^2)*data$W[k] + pmin(data$V3, 0) + data$unit.effect[i]+rnorm(1,mean=0,sd=1)
    }
}
data$y=y
data$unit <- unit

# Split between test and train data
data.train <- data %>%
    filter(train==1)
data.test <- data %>%
    filter(train==0)

ggplot(data, aes(x = V1, y = y)) + 
    geom_point()

```

Thus, we have indeed a non-linear relationship between $V1$ and the outcome $y$. Let's run the FE, RE and the OLS regressions using the train sample:

```{r}
# FE
fe <- felm(y ~ V1*W | unit, data=data.train)

# RE
re <- lmer(y ~ V1*W + (1 | unit), data=data.train)

# POLS
ols <- lm(y ~ V1*W, data = data.train)

# Summary 

summary(fe)
summary(re)
summary(ols)

```

Note that now we only have statistically significance on the treatment, but none in the heterogeneity, even knowing that our true model has a non-linear relationship between $V1$ and $W$. 

Let's see if the causal forest algorithm work well in this case:

```{r}
# Firm dummies
firm_dummies <- as.data.frame(model.matrix(y~as.factor(firm),data.train))

# Creating our train sample for X, Y and W

# Covariates
X <- as.matrix(bind_cols(data.train[,c(5,7:16)],firm_dummies))

str(X)

# Outcome
Y <- data.train$y

# Treatment
W <- data.train$W

# Train our causal forest with train test
tau.forest3 <- causal_forest(X,Y,W)

# Estimate ATE with all train sample
average_treatment_effect(tau.forest3, target.sample = "all")

# Estimate ATE with trated train sample
average_treatment_effect(tau.forest3, target.sample = "treated")

# Predicting CATE with Best Linear Prediction
cate_best_nl2 <- best_linear_projection(tau.forest3)

cate_best_nl2


```

Now let's predict our CATE with the test sample:

```{r}
# Create firm dummies for test sample
firm_dummies_test <- as.data.frame(model.matrix(y ~as.factor(firm), data.test))

# Create test set for X, Y and W, excluding V2
# Features
X.test <- as.matrix(bind_cols(data.test[,c(5,7:16)],firm_dummies_test))

 # Outcome
Y.test <- data.test$y

# Treatment
W.test <- data.test$W

# Predicting CATE (Again, form some reason we are having problems with columns. But to have full honesty, we should predict with the test sample, not the train one. But since this tutorial is only for learning, let's do with the train)
tau.hat3 <- predict(tau.forest3, estimate.variance = TRUE)

# Predicting variance
sigma.hat3 = sqrt(tau.hat3$variance.estimates)

# Extracting CATE
data.train$pred <- tau.hat3$predictions

# Histogram of CATE
hist(data.train$pred)

# Plotting CATE vs V1. It should be non-linear
ggplot(data.train, aes(x=V1, y = pred)) +
    geom_point()

```

Thus, it seems that our CATE prediction from the causal forest has indeed a non-linear relationship with $V1$. Let's compare the real CATE with the prediction:

```{r}
# Storing out CATE info in a dataframe
graph.data3 <- as.data.frame(tau.hat3)

# Create cate and CI bounders
graph.data3 <- graph.data3 %>%
    mutate(cate=pmax(data.train$V1,0)) %>%
    mutate(lower=predictions - sigma.hat3, upper=predictions + sigma.hat3)

# Plot the relationship between the predicted CATE and the real CATE
ggplot(graph.data3, aes(x=cate, y=predictions)) + geom_point() 
```
Which seems that the CATE's causal forest prediction matches with the real one. Finally, with we look at the error plot:

```{r}
# Plot error plot with standard errors
ggplot() + geom_errorbar(graph.data3, mapping=aes(x=cate,  ymin=lower,ymax=upper), alpha = 0.2) + geom_point(graph.data3, mapping=aes(x=cate,  y=predictions), alpha = 0.2)
```

That is, our prediction in fact has a non-linear relationship with the CATE, which corresponds with our real model. 

Is interesting to see that the CATE estimated by the causal forest algorithm, even not performing good for simpler models (See Tutorial 1), brings us good results when there is a clear non-linear relationship between the covariate and the treatment. The next step is to do with observational data, rather than randomized. 

\section{References}

Tutorial 1 - Estimating Heterogeneous Treatment Effects in Randomized Data with Machine Learning Techniques (available at https://sites.google.com/view/victor-hugo-alexandrino/)

https://cran.r-project.org/web/packages/SuperLearner/vignettes/Guide-to-SuperLearner.html

https://gsbdbi.github.io/ml_tutorial/

https://ml-in-econ.appspot.com

https://github.com/QuantLet/Meta_learner-for-Causal-ML/blob/main/GRF/Causal-Forest.R

https://grf-labs.github.io/grf/

https://www.markhw.com/blog/causalforestintro

https://lost-stats.github.io/Machine_Learning/causal_forest.html

